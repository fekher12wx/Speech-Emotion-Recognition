# -*- coding: utf-8 -*-
"""deep_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19daa8r_3k_CXzO8ReiQiyr4QP0O4ixZb
"""

# ============================================
# SPEECH EMOTION RECOGNITION - GOOGLE COLAB
# With Automatic Kaggle Dataset Download
# ============================================

import tensorflow as tf


# ============================================
# CELL 2: MOUNT GOOGLE DRIVE & SETUP KAGGLE API
# ============================================
print("\nüîë Setting up Kaggle API from Google Drive...")
print("="*70)

from google.colab import drive
import os
import shutil

# Mount Google Drive
print("\nüìÇ Mounting Google Drive...")
drive.mount('/content/drive')
print("‚úÖ Drive mounted!")

# Copy kaggle.json from Drive to Colab
print("\nüîë Setting up Kaggle credentials...")

# UPDATE THIS PATH to where you saved kaggle.json in your Drive
KAGGLE_JSON_PATH = "/content/drive/MyDrive/kaggle.json"

# KAGGLE_JSON_PATH = "/content/drive/MyDrive/Colab Notebooks/kaggle.json"
# KAGGLE_JSON_PATH = "/content/drive/MyDrive/keys/kaggle.json"

# Check if kaggle.json exists
if os.path.exists(KAGGLE_JSON_PATH):
    print(f"‚úÖ Found kaggle.json at: {KAGGLE_JSON_PATH}")

    # Setup Kaggle credentials
    !mkdir -p ~/.kaggle
    shutil.copy(KAGGLE_JSON_PATH, '/root/.kaggle/kaggle.json')
    !chmod 600 ~/.kaggle/kaggle.json

    print("‚úÖ Kaggle API configured!")
else:
    print(f"‚ùå kaggle.json NOT FOUND at: {KAGGLE_JSON_PATH}")
    print("\nüìù Please:")
    print("1. Upload kaggle.json to your Google Drive")
    print("2. Update KAGGLE_JSON_PATH in the code above")
    print("\nCurrent Drive contents:")
    !ls -la /content/drive/MyDrive/ | head -20

print("="*70)

# ============================================
# CELL 3: DOWNLOAD DATASETS FROM KAGGLE
# ============================================
print("\nüì¶ Downloading datasets from Kaggle...")
print("="*70)

# Create data directory
!mkdir -p /content/data

# Download RAVDESS Dataset
print("\n1Ô∏è‚É£ Downloading RAVDESS...")
!kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio -p /content/data --unzip
print("‚úÖ RAVDESS downloaded")

# Download CREMA-D Dataset
print("\n2Ô∏è‚É£ Downloading CREMA-D...")
!kaggle datasets download -d ejlok1/cremad -p /content/data --unzip
print("‚úÖ CREMA-D downloaded")

# Download TESS Dataset
print("\n3Ô∏è‚É£ Downloading TESS...")
!kaggle datasets download -d ejlok1/toronto-emotional-speech-set-tess -p /content/data --unzip
print("‚úÖ TESS downloaded")

# Download SAVEE Dataset
print("\n4Ô∏è‚É£ Downloading SAVEE...")
!kaggle datasets download -d ejlok1/surrey-audiovisual-expressed-emotion-savee -p /content/data --unzip
print("‚úÖ SAVEE downloaded")

print("\n" + "="*70)
print("‚úÖ ALL DATASETS DOWNLOADED!")
print("="*70)

# List downloaded files
print("\nüìÇ Checking downloaded files:")
!ls -lh /content/data/

# ============================================
# CELL 4: ORGANIZE DATASET STRUCTURE
# ============================================
print("\nüìÅ Organizing datasets...")

import os
import shutil

# Create organized structure
base_path = "/content/data/"
organized_path = "/content/organized_data/"

!mkdir -p {organized_path}Ravdess
!mkdir -p {organized_path}Crema
!mkdir -p {organized_path}Tess
!mkdir -p {organized_path}Savee

# Function to organize files
def organize_datasets():
    """Organize downloaded datasets into proper structure"""

    # Find and move RAVDESS files
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.wav'):
                file_path = os.path.join(root, file)

                # RAVDESS (Actor_XX format)
                if 'Actor_' in root or file.startswith('03-'):
                    dest = os.path.join(organized_path, 'Ravdess', file)
                    if not os.path.exists(dest):
                        shutil.copy2(file_path, dest)

                # CREMA-D (contains underscore pattern like 1001_DFA_ANG_XX)
                elif '_' in file and any(x in file for x in ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']):
                    dest = os.path.join(organized_path, 'Crema', file)
                    if not os.path.exists(dest):
                        shutil.copy2(file_path, dest)

                # TESS (contains emotion words)
                elif any(emotion in file.lower() for emotion in ['angry', 'disgust', 'fear', 'happy', 'neutral', 'ps', 'sad', 'pleasant']):
                    dest = os.path.join(organized_path, 'Tess', file)
                    if not os.path.exists(dest):
                        shutil.copy2(file_path, dest)

                # SAVEE (starts with letter codes like 'a', 'd', 'f', etc.)
                elif file[0].lower() in ['a', 'd', 'f', 'h', 'n', 's'] and len(file) < 15:
                    dest = os.path.join(organized_path, 'Savee', file)
                    if not os.path.exists(dest):
                        shutil.copy2(file_path, dest)

organize_datasets()

# Verify organization
print("\n‚úÖ Dataset organization complete!")
print("\nüìä File counts:")
for dataset in ['Ravdess', 'Crema', 'Tess', 'Savee']:
    path = os.path.join(organized_path, dataset)
    count = len([f for f in os.listdir(path) if f.endswith('.wav')])
    print(f"  {dataset:10s}: {count:4d} files")

# Update paths
DATA_PATH = organized_path
Ravdess = DATA_PATH + "Ravdess/"
Crema = DATA_PATH + "Crema/"
Tess = DATA_PATH + "Tess/"
Savee = DATA_PATH + "Savee/"

# ============================================
# CELL 5: INSTALL & IMPORT LIBRARIES
# ============================================
print("\nüì¶ Installing libraries...")
!pip install -q librosa

import numpy as np
import pandas as pd
import librosa
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import warnings
warnings.filterwarnings('ignore')

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical

print("‚úÖ All libraries imported!")

# ============================================
# CELL 6: ENABLE MIXED PRECISION
# ============================================
print("\n‚ö° Enabling Mixed Precision Training...")

try:
    policy = tf.keras.mixed_precision.Policy('mixed_float16')
    tf.keras.mixed_precision.set_global_policy(policy)
    print(f"‚úÖ Mixed Precision: {policy.name}")
    print(f"   Compute: {policy.compute_dtype} | Variable: {policy.variable_dtype}")
except Exception as e:
    print(f"‚ö†Ô∏è Mixed precision error: {e}")

# ============================================
# CELL 7: EMOTION EXTRACTION FUNCTIONS
# ============================================
def extract_emotion_ravdess(filename):
    emotion_dict = {
        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',
        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'
    }
    try:
        emotion_code = filename.split('-')[2]
        return emotion_dict.get(emotion_code, 'unknown')
    except:
        return 'unknown'

def extract_emotion_crema(filename):
    emotion_dict = {
        'ANG': 'angry', 'DIS': 'disgust', 'FEA': 'fearful',
        'HAP': 'happy', 'NEU': 'neutral', 'SAD': 'sad'
    }
    try:
        emotion_code = filename.split('_')[2]
        return emotion_dict.get(emotion_code, 'unknown')
    except:
        return 'unknown'

def extract_emotion_tess(filename):
    emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'ps', 'sad']
    filename_lower = filename.lower()
    for emotion in emotions:
        if emotion in filename_lower:
            return 'surprised' if emotion == 'ps' else emotion
    return 'unknown'

def extract_emotion_savee(filename):
    emotion_dict = {
        'a': 'angry', 'd': 'disgust', 'f': 'fearful',
        'h': 'happy', 'n': 'neutral', 'sa': 'sad', 'su': 'surprised'
    }
    try:
        emotion_code = filename[0:2] if len(filename) > 1 and filename[1].isalpha() else filename[0]
        return emotion_dict.get(emotion_code.lower(), 'unknown')
    except:
        return 'unknown'

print("‚úÖ Emotion extraction functions ready")

def extract_features(file_path, duration=3, offset=0.5):
    """Enhanced feature extraction"""
    try:
        audio, sr = librosa.load(file_path, duration=duration, offset=offset, sr=22050)

        # MFCC + Deltas
        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)
        mfccs_mean = np.mean(mfccs.T, axis=0)
        mfccs_std = np.std(mfccs.T, axis=0)
        mfccs_delta = librosa.feature.delta(mfccs)
        mfccs_delta_mean = np.mean(mfccs_delta.T, axis=0)

        # Chroma
        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)
        chroma_mean = np.mean(chroma.T, axis=0)
        chroma_std = np.std(chroma.T, axis=0)

        # Mel
        mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)
        mel_mean = np.mean(mel.T, axis=0)
        mel_std = np.std(mel.T, axis=0)

        # Contrast
        contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)
        contrast_mean = np.mean(contrast.T, axis=0)

        # Tonnetz
        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(audio), sr=sr)
        tonnetz_mean = np.mean(tonnetz.T, axis=0)

        # Prosodic features
        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sr))
        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sr))
        zcr = np.mean(librosa.feature.zero_crossing_rate(audio))
        rms = np.mean(librosa.feature.rms(y=audio))

        return np.concatenate([
            mfccs_mean, mfccs_std, mfccs_delta_mean,
            chroma_mean, chroma_std,
            mel_mean, mel_std,
            contrast_mean, tonnetz_mean,
            [spectral_centroid, spectral_rolloff, zcr, rms]
        ])
    except Exception as e:
        return None

# CELL 9: LOAD DATASETS
# ============================================
def load_dataset(data_path, dataset_name, emotion_extractor):
    X, y = [], []
    file_count = 0

    print(f"\nüìÇ Loading {dataset_name}...")

    for root, dirs, files in os.walk(data_path):
        for file in files:
            if file.endswith('.wav'):
                file_path = os.path.join(root, file)
                emotion = emotion_extractor(file)

                if emotion != 'unknown':
                    features = extract_features(file_path)
                    if features is not None:
                        X.append(features)
                        y.append(emotion)
                        file_count += 1
                        if file_count % 100 == 0:
                            print(f"  ‚úì {file_count} files processed...")

    print(f"‚úÖ {dataset_name}: {file_count} files")
    return X, y

print("\n" + "="*70)
print("üìä LOADING ALL DATASETS")
print("="*70)

X_all, y_all = [], []

if os.path.exists(Ravdess):
    X_rav, y_rav = load_dataset(Ravdess, "RAVDESS", extract_emotion_ravdess)
    X_all.extend(X_rav)
    y_all.extend(y_rav)

if os.path.exists(Crema):
    X_crema, y_crema = load_dataset(Crema, "CREMA-D", extract_emotion_crema)
    X_all.extend(X_crema)
    y_all.extend(y_crema)

if os.path.exists(Tess):
    X_tess, y_tess = load_dataset(Tess, "TESS", extract_emotion_tess)
    X_all.extend(X_tess)
    y_all.extend(y_tess)

if os.path.exists(Savee):
    X_savee, y_savee = load_dataset(Savee, "SAVEE", extract_emotion_savee)
    X_all.extend(X_savee)
    y_all.extend(y_savee)

X_all = np.array(X_all)
y_all = np.array(y_all)

print(f"\n{'='*70}")
print(f"‚úÖ TOTAL SAMPLES: {len(X_all)}")
print(f"üìè Features: {X_all.shape[1]}")
print(f"{'='*70}")

# ============================================
# CELL 10: DATA VISUALIZATION
# ============================================
df = pd.DataFrame({'emotion': y_all})

print("\nüìä Emotion Distribution:")
print(df['emotion'].value_counts())

plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='emotion', palette='Set2', order=df['emotion'].value_counts().index)
plt.title('Emotion Distribution', fontsize=16, fontweight='bold')
plt.xlabel('Emotion', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('emotion_distribution.png', dpi=150, bbox_inches='tight')
plt.show()

# ============================================
# CELL 11: PREPROCESSING
# ============================================
print("\nüîß Preprocessing...")

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y_all)
y_categorical = to_categorical(y_encoded)

print(f"Classes: {label_encoder.classes_}")
print(f"Number of classes: {len(label_encoder.classes_)}")

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_all)
X_reshaped = np.expand_dims(X_scaled, axis=2)

X_train, X_test, y_train, y_test = train_test_split(
    X_reshaped, y_categorical,
    test_size=0.2,
    random_state=42,
    stratify=y_categorical
)

print(f"\n‚úÖ Train: {X_train.shape}")
print(f"‚úÖ Test: {X_test.shape}")

from tensorflow.keras.layers import GlobalAveragePooling1D

# ============================================
# DATA AUGMENTATION (Add noise and shifts)
# ============================================
def augment_audio(X, y, augment_factor=2):
    """Augment audio data with noise and time shifts"""
    X_aug = []
    y_aug = []

    for i in range(len(X)):
        # Original sample
        X_aug.append(X[i])
        y_aug.append(y[i])

        # Add augmented samples
        for _ in range(augment_factor - 1):
            sample = X[i].copy()

            # Add random noise
            noise = np.random.normal(0, 0.005, sample.shape)
            sample = sample + noise

            # Random time shift
            shift = np.random.randint(-20, 20)
            sample = np.roll(sample, shift, axis=0)

            X_aug.append(sample)
            y_aug.append(y[i])

    return np.array(X_aug), np.array(y_aug)

print("Augmenting training data...")
X_train_aug, y_train_aug = augment_audio(X_train, y_train, augment_factor=2)
print(f"Original: {X_train.shape}, Augmented: {X_train_aug.shape}")

# ============================================
# IMPROVED MODEL ARCHITECTURE
# ============================================
def create_model(input_shape, num_classes):
    model = Sequential([
        # Block 1
        Conv1D(64, 5, padding='same', input_shape=input_shape),
        BatchNormalization(),
        Activation('relu'),
        MaxPooling1D(2),
        Dropout(0.2),

        # Block 2
        Conv1D(128, 5, padding='same'),
        BatchNormalization(),
        Activation('relu'),
        MaxPooling1D(2),
        Dropout(0.2),

        # Block 3
        Conv1D(256, 3, padding='same'),
        BatchNormalization(),
        Activation('relu'),
        MaxPooling1D(2),
        Dropout(0.3),

        # Block 4
        Conv1D(512, 3, padding='same'),
        BatchNormalization(),
        Activation('relu'),
        MaxPooling1D(2),
        Dropout(0.3),

        # Global pooling
        GlobalAveragePooling1D(),

        # Dense
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),

        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.4),

        Dense(num_classes, activation='softmax')
    ])
    return model

model = create_model(input_shape=(417, 1), num_classes=9)
model.summary()

# ============================================
# CREATE MODEL
# ============================================
model = create_model(input_shape=(417, 1), num_classes=9)
model.summary()

# ============================================
# SETUP CALLBACKS
# ============================================
checkpoint = ModelCheckpoint(
    'best_emotion_model_v2.keras',
    monitor='val_accuracy',
    mode='max',
    save_best_only=True,
    verbose=1
)

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=30,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=10,
    min_lr=0.00001,
    verbose=1
)

callbacks = [checkpoint, early_stop, reduce_lr]

# ============================================
# CLASS WEIGHTS
# ============================================
from sklearn.utils.class_weight import compute_class_weight

y_train_labels = np.argmax(y_train_aug, axis=1)

class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(y_train_labels),
    y=y_train_labels
)
class_weight_dict = dict(enumerate(class_weights))

print("\nClass weights:", class_weight_dict)

# ============================================
# COMPILE MODEL
# ============================================
model.compile(
    loss='categorical_crossentropy',
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)

# ============================================
# TRAIN MODEL
# ============================================
history = model.fit(
    X_train_aug, y_train_aug,  # Use augmented data
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=64,
    class_weight=class_weight_dict,
    callbacks=callbacks,
    verbose=1
)

# ============================================
# CELL 15: EVALUATE
# ============================================
print("\nüìà Evaluating...")

model = keras.models.load_model('best_emotion_model.keras')

y_pred = model.predict(X_test, batch_size=64)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

accuracy = accuracy_score(y_true, y_pred_classes)
print(f"\nüéØ Accuracy: {accuracy * 100:.2f}%")

print("\n" + "="*70)
print("Classification Report")
print("="*70)
print(classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_))

# ============================================
# CELL 16: VISUALIZATIONS
# ============================================
# Confusion Matrix
cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.show()

# Training History
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train', linewidth=2)
plt.plot(history.history['val_accuracy'], label='Validation', linewidth=2)
plt.title('Accuracy', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train', linewidth=2)
plt.plot(history.history['val_loss'], label='Validation', linewidth=2)
plt.title('Loss', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
plt.show()

# ============================================
# CELL 17: PREDICTION FUNCTION
# ============================================
def predict_emotion(audio_file_path):
    """Predict emotion from audio"""
    features = extract_features(audio_file_path)
    if features is None:
        return "Error", 0.0

    features = scaler.transform([features])
    features = np.expand_dims(features, axis=2)

    prediction = model.predict(features, verbose=0)
    emotion = label_encoder.classes_[np.argmax(prediction)]
    confidence = np.max(prediction) * 100

    return emotion, confidence

print("‚úÖ Prediction function ready!")
print("\nUsage: emotion, conf = predict_emotion('audio.wav')")

# ============================================
# SAVE PREPROCESSING OBJECTS FOR WEB APP
# ============================================
print("\nüíæ Saving preprocessing objects for web application...")
import pickle

try:
    with open('scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    print("‚úÖ Scaler saved to scaler.pkl")
    
    with open('label_encoder.pkl', 'wb') as f:
        pickle.dump(label_encoder, f)
    print("‚úÖ Label encoder saved to label_encoder.pkl")
except Exception as e:
    print(f"‚ö†Ô∏è Error saving preprocessing objects: {e}")

# ============================================
# CELL 18: DOWNLOAD RESULTS
# ============================================
print("\nüíæ Downloading results...")

from google.colab import files

files.download('best_model.keras')
files.download('emotion_distribution.png')
files.download('confusion_matrix.png')
files.download('training_history.png')

print("\n‚úÖ All files downloaded!")

# ============================================
# CELL 19: SUMMARY
# ============================================
print("\n" + "="*70)
print("‚úÖ TRAINING COMPLETE!")
print("="*70)
if 'accuracy' in locals():
    print(f"üéØ Accuracy: {accuracy * 100:.2f}%")
print(f"üì¶ Batch Size: 64")
print(f"üéõÔ∏è  Mixed Precision: Enabled")
print(f"\nüìÅ Files Generated:")
print("  ‚Ä¢ best_emotion_model_v2.keras (or best_emotion_model.keras)")
print("  ‚Ä¢ scaler.pkl")
print("  ‚Ä¢ label_encoder.pkl")
print("  ‚Ä¢ emotion_distribution.png")
print("  ‚Ä¢ confusion_matrix.png")
print("  ‚Ä¢ training_history.png")
print("="*70)
print("\nüéâ SUCCESS! Your model is trained and ready!")
print("üí° You can now run the web app with: streamlit run app.py")
print("="*70)