# -*- coding: utf-8 -*-
"""deep with resultat 64.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11rzbuDDvc3cKVnA9zWpWa5kRn0ido4Ky
"""

# ============================================
# SPEECH EMOTION RECOGNITION - GOOGLE COLAB
# With Automatic Kaggle Dataset Download
# ============================================

import tensorflow as tf


# ============================================
# CELL 2: MOUNT GOOGLE DRIVE & SETUP KAGGLE API
# ============================================
print("\nüîë Setting up Kaggle API from Google Drive...")
print("="*70)

from google.colab import drive
import os
import shutil

# Mount Google Drive
print("\nüìÇ Mounting Google Drive...")
drive.mount('/content/drive')
print("‚úÖ Drive mounted!")

# Copy kaggle.json from Drive to Colab
print("\nüîë Setting up Kaggle credentials...")

# UPDATE THIS PATH to where you saved kaggle.json in your Drive
KAGGLE_JSON_PATH = "/content/drive/MyDrive/kaggle.json"

# Alternative common paths (uncomment the one that matches your setup):
# KAGGLE_JSON_PATH = "/content/drive/MyDrive/Colab Notebooks/kaggle.json"
# KAGGLE_JSON_PATH = "/content/drive/MyDrive/keys/kaggle.json"

# Check if kaggle.json exists
if os.path.exists(KAGGLE_JSON_PATH):
    print(f"‚úÖ Found kaggle.json at: {KAGGLE_JSON_PATH}")

    # Setup Kaggle credentials
    !mkdir -p ~/.kaggle
    shutil.copy(KAGGLE_JSON_PATH, '/root/.kaggle/kaggle.json')
    !chmod 600 ~/.kaggle/kaggle.json

    print("‚úÖ Kaggle API configured!")
else:
    print(f"‚ùå kaggle.json NOT FOUND at: {KAGGLE_JSON_PATH}")
    print("\nüìù Please:")
    print("1. Upload kaggle.json to your Google Drive")
    print("2. Update KAGGLE_JSON_PATH in the code above")
    print("\nCurrent Drive contents:")
    !ls -la /content/drive/MyDrive/ | head -20

print("="*70)

# ============================================
# CELL 3: DOWNLOAD DATASETS FROM KAGGLE
# ============================================
print("\nüì¶ Downloading datasets from Kaggle...")
print("="*70)

# Create data directory
!mkdir -p /content/data

# Download RAVDESS Dataset
print("\n1Ô∏è‚É£ Downloading RAVDESS...")
!kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio -p /content/data --unzip
print("‚úÖ RAVDESS downloaded")

# Download CREMA-D Dataset
print("\n2Ô∏è‚É£ Downloading CREMA-D...")
!kaggle datasets download -d ejlok1/cremad -p /content/data --unzip
print("‚úÖ CREMA-D downloaded")

# Download TESS Dataset
print("\n3Ô∏è‚É£ Downloading TESS...")
!kaggle datasets download -d ejlok1/toronto-emotional-speech-set-tess -p /content/data --unzip
print("‚úÖ TESS downloaded")

# Download SAVEE Dataset
print("\n4Ô∏è‚É£ Downloading SAVEE...")
!kaggle datasets download -d ejlok1/surrey-audiovisual-expressed-emotion-savee -p /content/data --unzip
print("‚úÖ SAVEE downloaded")

print("\n" + "="*70)
print("‚úÖ ALL DATASETS DOWNLOADED!")
print("="*70)

# List downloaded files
print("\nüìÇ Checking downloaded files:")
!ls -lh /content/data/

# ============================================
# CELL 4: ORGANIZE DATASET STRUCTURE
# ============================================
print("\nüìÅ Organizing datasets...")

import os
import shutil

# Create organized structure
base_path = "/content/data/"
organized_path = "/content/organized_data/"

!mkdir -p {organized_path}Ravdess
!mkdir -p {organized_path}Crema
!mkdir -p {organized_path}Tess
!mkdir -p {organized_path}Savee

# Function to organize files
def organize_datasets():
    """Organize downloaded datasets into proper structure"""

    # Find and move RAVDESS files
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.wav'):
                file_path = os.path.join(root, file)

                # RAVDESS (Actor_XX format)
                if 'Actor_' in root or file.startswith('03-'):
                    dest = os.path.join(organized_path, 'Ravdess', file)
                    if not os.path.exists(dest):
                        shutil.copy2(file_path, dest)

                # CREMA-D (contains underscore pattern like 1001_DFA_ANG_XX)
                elif '_' in file and any(x in file for x in ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']):
                    dest = os.path.join(organized_path, 'Crema', file)
                    if not os.path.exists(dest):
                        shutil.copy2(file_path, dest)

                # TESS (contains emotion words)
                elif any(emotion in file.lower() for emotion in ['angry', 'disgust', 'fear', 'happy', 'neutral', 'ps', 'sad', 'pleasant']):
                    dest = os.path.join(organized_path, 'Tess', file)
                    if not os.path.exists(dest):
                        shutil.copy2(file_path, dest)

                # SAVEE (starts with letter codes like 'a', 'd', 'f', etc.)
                elif file[0].lower() in ['a', 'd', 'f', 'h', 'n', 's'] and len(file) < 15:
                    dest = os.path.join(organized_path, 'Savee', file)
                    if not os.path.exists(dest):
                        shutil.copy2(file_path, dest)

organize_datasets()

# Verify organization
print("\n‚úÖ Dataset organization complete!")
print("\nüìä File counts:")
for dataset in ['Ravdess', 'Crema', 'Tess', 'Savee']:
    path = os.path.join(organized_path, dataset)
    count = len([f for f in os.listdir(path) if f.endswith('.wav')])
    print(f"  {dataset:10s}: {count:4d} files")

# Update paths
DATA_PATH = organized_path
Ravdess = DATA_PATH + "Ravdess/"
Crema = DATA_PATH + "Crema/"
Tess = DATA_PATH + "Tess/"
Savee = DATA_PATH + "Savee/"

# ============================================
# CELL 5: INSTALL & IMPORT LIBRARIES
# ============================================
print("\nüì¶ Installing libraries...")
!pip install -q librosa

import numpy as np
import pandas as pd
import librosa
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import warnings
warnings.filterwarnings('ignore')

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical

print("‚úÖ All libraries imported!")

# ============================================
# CELL 6: ENABLE MIXED PRECISION
# ============================================
print("\n‚ö° Enabling Mixed Precision Training...")

try:
    policy = tf.keras.mixed_precision.Policy('mixed_float16')
    tf.keras.mixed_precision.set_global_policy(policy)
    print(f"‚úÖ Mixed Precision: {policy.name}")
    print(f"   Compute: {policy.compute_dtype} | Variable: {policy.variable_dtype}")
except Exception as e:
    print(f"‚ö†Ô∏è Mixed precision error: {e}")

# ============================================
# CELL 7: EMOTION EXTRACTION FUNCTIONS
# ============================================
def extract_emotion_ravdess(filename):
    emotion_dict = {
        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',
        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'
    }
    try:
        emotion_code = filename.split('-')[2]
        return emotion_dict.get(emotion_code, 'unknown')
    except:
        return 'unknown'

def extract_emotion_crema(filename):
    emotion_dict = {
        'ANG': 'angry', 'DIS': 'disgust', 'FEA': 'fearful',
        'HAP': 'happy', 'NEU': 'neutral', 'SAD': 'sad'
    }
    try:
        emotion_code = filename.split('_')[2]
        return emotion_dict.get(emotion_code, 'unknown')
    except:
        return 'unknown'

def extract_emotion_tess(filename):
    emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'ps', 'sad']
    filename_lower = filename.lower()
    for emotion in emotions:
        if emotion in filename_lower:
            return 'surprised' if emotion == 'ps' else emotion
    return 'unknown'

def extract_emotion_savee(filename):
    emotion_dict = {
        'a': 'angry', 'd': 'disgust', 'f': 'fearful',
        'h': 'happy', 'n': 'neutral', 'sa': 'sad', 'su': 'surprised'
    }
    try:
        emotion_code = filename[0:2] if len(filename) > 1 and filename[1].isalpha() else filename[0]
        return emotion_dict.get(emotion_code.lower(), 'unknown')
    except:
        return 'unknown'

print("‚úÖ Emotion extraction functions ready")

# ============================================
# CELL 8: FEATURE EXTRACTION
# ============================================
def extract_features(file_path, duration=3, offset=0.5):
    """Extract audio features"""
    try:
        audio, sr = librosa.load(file_path, duration=duration, offset=offset)

        mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40).T, axis=0)
        chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sr).T, axis=0)
        mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sr).T, axis=0)
        contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sr).T, axis=0)
        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(audio), sr=sr).T, axis=0)

        return np.concatenate([mfccs, chroma, mel, contrast, tonnetz])
    except Exception as e:
        return None

print("‚úÖ Feature extraction function ready")

# ============================================

# CELL 9: LOAD DATASETS
# ============================================
def load_dataset(data_path, dataset_name, emotion_extractor):
    X, y = [], []
    file_count = 0

    print(f"\nüìÇ Loading {dataset_name}...")

    for root, dirs, files in os.walk(data_path):
        for file in files:
            if file.endswith('.wav'):
                file_path = os.path.join(root, file)
                emotion = emotion_extractor(file)

                if emotion != 'unknown':
                    features = extract_features(file_path)
                    if features is not None:
                        X.append(features)
                        y.append(emotion)
                        file_count += 1
                        if file_count % 100 == 0:
                            print(f"  ‚úì {file_count} files processed...")

    print(f"‚úÖ {dataset_name}: {file_count} files")
    return X, y

print("\n" + "="*70)
print("üìä LOADING ALL DATASETS")
print("="*70)

X_all, y_all = [], []

if os.path.exists(Ravdess):
    X_rav, y_rav = load_dataset(Ravdess, "RAVDESS", extract_emotion_ravdess)
    X_all.extend(X_rav)
    y_all.extend(y_rav)

if os.path.exists(Crema):
    X_crema, y_crema = load_dataset(Crema, "CREMA-D", extract_emotion_crema)
    X_all.extend(X_crema)
    y_all.extend(y_crema)

if os.path.exists(Tess):
    X_tess, y_tess = load_dataset(Tess, "TESS", extract_emotion_tess)
    X_all.extend(X_tess)
    y_all.extend(y_tess)

if os.path.exists(Savee):
    X_savee, y_savee = load_dataset(Savee, "SAVEE", extract_emotion_savee)
    X_all.extend(X_savee)
    y_all.extend(y_savee)

X_all = np.array(X_all)
y_all = np.array(y_all)

print(f"\n{'='*70}")
print(f"‚úÖ TOTAL SAMPLES: {len(X_all)}")
print(f"üìè Features: {X_all.shape[1]}")
print(f"{'='*70}")

# ============================================
# CELL 10: DATA VISUALIZATION
# ============================================
df = pd.DataFrame({'emotion': y_all})

print("\nüìä Emotion Distribution:")
print(df['emotion'].value_counts())

plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='emotion', palette='Set2', order=df['emotion'].value_counts().index)
plt.title('Emotion Distribution', fontsize=16, fontweight='bold')
plt.xlabel('Emotion', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('emotion_distribution.png', dpi=150, bbox_inches='tight')
plt.show()

# ============================================
# CELL 11: PREPROCESSING
# ============================================
print("\nüîß Preprocessing...")

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y_all)
y_categorical = to_categorical(y_encoded)

print(f"Classes: {label_encoder.classes_}")
print(f"Number of classes: {len(label_encoder.classes_)}")

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_all)
X_reshaped = np.expand_dims(X_scaled, axis=2)

X_train, X_test, y_train, y_test = train_test_split(
    X_reshaped, y_categorical,
    test_size=0.2,
    random_state=42,
    stratify=y_categorical
)

print(f"\n‚úÖ Train: {X_train.shape}")
print(f"‚úÖ Test: {X_test.shape}")

# ============================================
# CELL 12: BUILD MODEL
# ============================================
print("\nüèóÔ∏è Building model...")

def create_model(input_shape, num_classes):
    model = Sequential([
        Conv1D(256, 8, padding='same', input_shape=input_shape),
        Activation('relu'),
        BatchNormalization(),
        MaxPooling1D(pool_size=4),
        Dropout(0.3),

        Conv1D(128, 8, padding='same'),
        Activation('relu'),
        BatchNormalization(),
        MaxPooling1D(pool_size=4),
        Dropout(0.3),

        Conv1D(64, 8, padding='same'),
        Activation('relu'),
        BatchNormalization(),
        MaxPooling1D(pool_size=2),
        Dropout(0.3),

        Flatten(),
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),

        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),

        Dense(num_classes, activation='softmax', dtype='float32')
    ])
    return model

model = create_model((X_train.shape[1], 1), y_train.shape[1])

model.compile(
    loss='categorical_crossentropy',
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)

print(model.summary())

# ============================================
# CELL 13: SETUP CALLBACKS
# ============================================
checkpoint = ModelCheckpoint(
    'best_model.keras',
    monitor='val_accuracy',
    mode='max',
    save_best_only=True,
    verbose=1
)

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=0.00001,
    verbose=1
)

callbacks = [checkpoint, early_stop, reduce_lr]

# ============================================
# CELL 14: TRAIN MODEL
# ============================================
print("\n" + "="*70)
print("üöÄ TRAINING MODEL")
print("="*70)

import time
start_time = time.time()

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=50,
    batch_size=64,
    callbacks=callbacks,
    verbose=1
)

training_time = time.time() - start_time
print(f"\n‚è±Ô∏è Training: {training_time/60:.2f} minutes")

# ============================================
# CELL 15: EVALUATE
# ============================================
print("\nüìà Evaluating...")

model = keras.models.load_model('best_model.keras')

y_pred = model.predict(X_test, batch_size=64)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

accuracy = accuracy_score(y_true, y_pred_classes)
print(f"\nüéØ Accuracy: {accuracy * 100:.2f}%")

print("\n" + "="*70)
print("Classification Report")
print("="*70)
print(classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_))

# ============================================
# CELL 16: VISUALIZATIONS
# ============================================
# Confusion Matrix
cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.show()

# Training History
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train', linewidth=2)
plt.plot(history.history['val_accuracy'], label='Validation', linewidth=2)
plt.title('Accuracy', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train', linewidth=2)
plt.plot(history.history['val_loss'], label='Validation', linewidth=2)
plt.title('Loss', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
plt.show()

# ============================================
# CELL 17: PREDICTION FUNCTION
# ============================================
def predict_emotion(audio_file_path):
    """Predict emotion from audio"""
    features = extract_features(audio_file_path)
    if features is None:
        return "Error", 0.0

    features = scaler.transform([features])
    features = np.expand_dims(features, axis=2)

    prediction = model.predict(features, verbose=0)
    emotion = label_encoder.classes_[np.argmax(prediction)]
    confidence = np.max(prediction) * 100

    return emotion, confidence

print("‚úÖ Prediction function ready!")
print("\nUsage: emotion, conf = predict_emotion('audio.wav')")

# ============================================
# CELL 18: DOWNLOAD RESULTS
# ============================================
print("\nüíæ Downloading results...")

from google.colab import files

files.download('best_model.keras')
files.download('emotion_distribution.png')
files.download('confusion_matrix.png')
files.download('training_history.png')

print("\n‚úÖ All files downloaded!")

# ============================================
# CELL 19: SUMMARY
# ============================================
print("\n" + "="*70)
print("‚úÖ TRAINING COMPLETE!")
print("="*70)
print(f"‚è±Ô∏è  Time: {training_time/60:.2f} minutes")
print(f"üéØ Accuracy: {accuracy * 100:.2f}%")
print(f"üì¶ Batch Size: 64")
print(f"üéõÔ∏è  Mixed Precision: Enabled")
print(f"\nüìÅ Files Generated:")
print("  ‚Ä¢ best_model.keras")
print("  ‚Ä¢ emotion_distribution.png")
print("  ‚Ä¢ confusion_matrix.png")
print("  ‚Ä¢ training_history.png")
print("="*70)
print("\nüéâ SUCCESS! Your model is trained and ready!")
print("="*70)